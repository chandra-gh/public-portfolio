{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Arabian Nights\\nIn the chronicles of the ancien'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Arabian nights\n",
    "file = 'arabian-nights.txt'\n",
    "corpus = None\n",
    "with open(file, 'r') as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "corpus[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[1], [1, 4049], [10], [10, 1], [10, 1, 4050]], [4049, 1853, 1, 4050, 4])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the content\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "seq_len = 10\n",
    "\n",
    "def generate_training_data(corpus, seq_len):\n",
    "    # Populate the tokenizer vocabulary\n",
    "    tokenizer.fit_on_texts([corpus])\n",
    "    \n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    \n",
    "    for line in corpus.split('\\n')[:1000]:\n",
    "        if len(line.strip()) == 0:\n",
    "            continue\n",
    "        tokens = tokenizer.texts_to_sequences([line])[0]\n",
    "        if len(tokens) == 0:\n",
    "            continue\n",
    "        for i in range(1, len(tokens)):\n",
    "            if i < seq_len:\n",
    "                seqX = tokens[:i]\n",
    "            else:\n",
    "                seqX = tokens[i-seq_len+1:i]\n",
    "            dataX.append(seqX)\n",
    "            dataY.append(tokens[i])\n",
    "    return dataX, dataY\n",
    "\n",
    "dataX, dataY = generate_training_data(corpus, seq_len)\n",
    "dataX[:5], dataY[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23366, 1)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad the input sequences\n",
    "paddedX = keras.preprocessing.sequence.pad_sequences(dataX, maxlen=seq_len)\n",
    "\n",
    "X = np.array(paddedX)\n",
    "Y = np.array(dataY).reshape((len(dataY), 1))\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 10, 10)            63530     \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 256)               273408    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 6353)              1632721   \n",
      "=================================================================\n",
      "Total params: 1,969,659\n",
      "Trainable params: 1,969,659\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "def create_model(vocab_size, seq_len):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Embedding(vocab_size, 10, input_length=seq_len))\n",
    "    # LSTM\n",
    "    model.add(keras.layers.LSTM(256))\n",
    "    # Regularization layer\n",
    "    model.add(keras.layers.Dropout(0.1))\n",
    "    # Dense softmax\n",
    "    model.add(keras.layers.Dense(vocab_size, activation=\"softmax\"))\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model(len(tokenizer.word_index)+1, seq_len)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "731/731 [==============================] - 31s 42ms/step - loss: 6.3547 - accuracy: 0.0639\n",
      "Epoch 2/10\n",
      "731/731 [==============================] - 41s 56ms/step - loss: 5.9022 - accuracy: 0.0761\n",
      "Epoch 3/10\n",
      "731/731 [==============================] - 37s 50ms/step - loss: 5.7054 - accuracy: 0.0838\n",
      "Epoch 4/10\n",
      "731/731 [==============================] - 41s 57ms/step - loss: 5.5511 - accuracy: 0.0891\n",
      "Epoch 5/10\n",
      "731/731 [==============================] - 44s 60ms/step - loss: 5.4087 - accuracy: 0.0963\n",
      "Epoch 6/10\n",
      "731/731 [==============================] - 40s 54ms/step - loss: 5.2565 - accuracy: 0.1040\n",
      "Epoch 7/10\n",
      "731/731 [==============================] - 41s 56ms/step - loss: 5.0783 - accuracy: 0.1121\n",
      "Epoch 8/10\n",
      "731/731 [==============================] - 37s 51ms/step - loss: 4.8743 - accuracy: 0.1213\n",
      "Epoch 9/10\n",
      "731/731 [==============================] - 39s 54ms/step - loss: 4.6596 - accuracy: 0.1299\n",
      "Epoch 10/10\n",
      "731/731 [==============================] - 41s 56ms/step - loss: 4.4294 - accuracy: 0.1400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8d76ecb5d0>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X, Y, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a found a pot of gold with some water in it and the genius of the genius and was and to the sultan who was very pleased to the sultan who was and to the sultan who was very pleased to the sultan who was and to the sultan who was very pleased to the sultan who was and to the sultan who was very pleased to the sultan who was and to the sultan who was very pleased to the sultan who was and to the sultan who was very pleased to the sultan who was and to the sultan who was very pleased to the sultan who was and']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text = 'a crow found a pot of gold with some water in it'\n",
    "gen_tokens = tokenizer.texts_to_sequences([seed_text])\n",
    "for i in range(100):\n",
    "    if len(gen_tokens[0]) > seq_len:\n",
    "        seed_tokens = np.array([gen_tokens[0][-seq_len:]])\n",
    "    else:\n",
    "        seed_tokens = keras.preprocessing.sequence.pad_sequences(gen_tokens, maxlen=seq_len)\n",
    "    preds = model.predict(seed_tokens)\n",
    "    index = np.argmax(preds)\n",
    "    gen_tokens[0].append(index)\n",
    "tokenizer.sequences_to_texts(gen_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
