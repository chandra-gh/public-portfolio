{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number Recognition\n",
    "\n",
    "Simple 2-layer model to recognize handwritten numbers.\n",
    "\n",
    "Model classifies 28x28 grayscale images from MNIST data set into one of the 10 digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input data\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data\n",
      "-------------\n",
      "Training data (60000, 28, 28)\n",
      "Training labels (60000,)\n",
      "Test data (10000, 28, 28)\n",
      "Test labels (10000,)\n",
      "\n",
      "Sample labels [5 0 4 1 9]\n",
      "\n",
      "Sample Image\n",
      "------------\n",
      "Expected label 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets take a peek at the data.\n",
    "print('Shape of data')\n",
    "print('-------------')\n",
    "print('Training data', x_train.shape)\n",
    "print('Training labels', y_train.shape)\n",
    "print('Test data', x_test.shape)\n",
    "print('Test labels', y_test.shape)\n",
    "print()\n",
    "print('Sample labels', y_train[:5])\n",
    "print()\n",
    "\n",
    "# Sample image\n",
    "index = 0\n",
    "print('Sample Image')\n",
    "print('------------')\n",
    "plt.imshow(x_train[index], cmap='gray')\n",
    "print('Expected label', y_train[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "* We have 60,000 28x28 grayscale images. Images are white on black background.\n",
    "* Y labels are not one-hot encoded.\n",
    "* We have 10,000 test images.\n",
    "\n",
    "We want the model to recognize both white numbers on black background and black numbers of a white background. Augment the data with inverted grayscale images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240000, 28, 28) (240000,) (40000, 28, 28) (40000,)\n"
     ]
    }
   ],
   "source": [
    "# Augment data with inverted grayscale images.\n",
    "x_train_inv = 255 - x_train\n",
    "x_test_inv = 255 - x_test\n",
    "\n",
    "x_train = np.append(x_train, x_train_inv, axis=0)\n",
    "y_train = np.append(y_train, y_train, axis=0)\n",
    "x_test = np.append(x_test, x_test_inv, axis=0)\n",
    "y_test = np.append(y_test, y_test, axis=0)\n",
    "\n",
    "# Print the new shape of train and test data.\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rationale behind the model\n",
    "\n",
    "Model performance vs. number of units in the hidden relu layer.\n",
    "\n",
    "||Units|Parameters|Train Acc.|Test Acc.|\n",
    "|--|--|--|--|--|\n",
    "|Model 1|128|~100,000|97%|95%|\n",
    "|Model 2|64|~51,000|97%|95%|\n",
    "|Model 3|32|~25,000|95%|94%|\n",
    "|Model 4|16|~12,000|90%|90%|\n",
    "\n",
    "Models 1 and 2 overfit the training set and are computationally expensive. \n",
    "Model 3 offers the best tradeoff - slight loss of accuracy for better computational performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "rescaling_7 (Rescaling)      (None, 28, 28)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 25,450\n",
      "Trainable params: 25,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "inputs = keras.Input(shape=(28, 28))\n",
    "\n",
    "# Normalize grayscale values.\n",
    "x = layers.experimental.preprocessing.Rescaling(1.0/255.)(inputs)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Use sparse_categorical_crossentropy instead of categorical_crossentropy because Y-labels are not one-hot encoded.\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7500/7500 [==============================] - 9s 1ms/step - loss: 0.6543 - accuracy: 0.7881\n",
      "Epoch 2/10\n",
      "7500/7500 [==============================] - 10s 1ms/step - loss: 0.3662 - accuracy: 0.8931\n",
      "Epoch 3/10\n",
      "7500/7500 [==============================] - 10s 1ms/step - loss: 0.3183 - accuracy: 0.9052\n",
      "Epoch 4/10\n",
      "7500/7500 [==============================] - 10s 1ms/step - loss: 0.2680 - accuracy: 0.9209\n",
      "Epoch 5/10\n",
      "7500/7500 [==============================] - 10s 1ms/step - loss: 0.2307 - accuracy: 0.9311\n",
      "Epoch 6/10\n",
      "7500/7500 [==============================] - 10s 1ms/step - loss: 0.2087 - accuracy: 0.9380\n",
      "Epoch 7/10\n",
      "7500/7500 [==============================] - 10s 1ms/step - loss: 0.1966 - accuracy: 0.9413\n",
      "Epoch 8/10\n",
      "7500/7500 [==============================] - 10s 1ms/step - loss: 0.1844 - accuracy: 0.9448\n",
      "Epoch 9/10\n",
      "7500/7500 [==============================] - 10s 1ms/step - loss: 0.1727 - accuracy: 0.9480\n",
      "Epoch 10/10\n",
      "7500/7500 [==============================] - 10s 1ms/step - loss: 0.1612 - accuracy: 0.9513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcf20e09710>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 970us/step - loss: 0.3458 - accuracy: 0.9018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34578704833984375, 0.9017999768257141]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on the test set.\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Expected: 4\n",
      "    Predicted: 4\n",
      "    Probability: 0.93\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOJklEQVR4nO3df6xU9ZnH8c8jhUSkRFguLljihQaTRcICXklV0rCp2/jjD+wfbUoU2MR4GyOxTaqWuIm9f2g0m6WkJmsJFSzVrg2BGtEYt4iNUI2NV8MKirsq3raUG+7gjwCJBtFn/7jHzRXvfM8w58yc4T7vVzKZmfPMmfPkcD+cmfmema+5uwCMfedU3QCA9iDsQBCEHQiCsANBEHYgiK+0c2PTpk3z7u7udm4SCGVgYEBHjx610WqFwm5mV0v6uaRxkh5y9/tTj+/u7lZ/f3+RTQJI6OnpqVtr+mW8mY2T9B+SrpE0T9IKM5vX7PMBaK0i79mXSHrb3Q+6+0lJv5W0vJy2AJStSNgvlPTXEfcPZcu+wMx6zazfzPprtVqBzQEookjYR/sQ4Evn3rr7Rnfvcfeerq6uApsDUESRsB+SNGvE/a9JOlysHQCtUiTsL0uaa2azzWyCpO9L2lFOWwDK1vTQm7ufMrM1kv5Lw0Nvm9399dI6A1CqQuPs7v60pKdL6gVAC3G6LBAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EUmsUV7XH48OFkfebMmW3qZOy44447kvV169Yl63fffXey3tfXd6YttVyhsJvZgKTjkj6VdMrde8poCkD5yjiy/5O7Hy3heQC0EO/ZgSCKht0l/d7MXjGz3tEeYGa9ZtZvZv21Wq3g5gA0q2jYr3T3xZKukXSrmX3z9Ae4+0Z373H3nq6uroKbA9CsQmF398PZ9ZCkxyUtKaMpAOVrOuxmdp6ZffXz25K+LWl/WY0BKFeRT+MvkPS4mX3+PP/p7s+U0lUwjzzySLJ+2223Jevz5s1r+rnnzJmTrI9VH374YbKe/V3XNTAwUGI37dF02N39oKR/LLEXAC3E0BsQBGEHgiDsQBCEHQiCsANB8BXXDrBv375k/dixY8n6Sy+9VLe2aNGi5Lo333xzsn7fffcl6+PHj0/Wq5QaHtu6dWty3XPOSR8HJ0+e3ExLleLIDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBMM4+BvT2jvqLYJKkd955J7nu+vXrk/VVq1Yl6wsWLEjWq7Rhw4a6tRMnTiTXveGGG5L1Bx54oKmeqsSRHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCYJy9Dd57771kfdOmTcn6hAkTkvXUd9IXL16cXHcsS0117e7JdfN+B+BsxJEdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4JgnL0NbrnllmQ9b/rgNWvWJOuRx9JTtm/fXrc2ceLE5LrXXXdd2e1ULvfIbmabzWzIzPaPWDbVzHaa2VvZ9ZTWtgmgqEZexv9K0tWnLVsraZe7z5W0K7sPoIPlht3dd0t6/7TFyyVtyW5vkXR9yX0BKFmzH9Bd4O6DkpRdT6/3QDPrNbN+M+uv1WpNbg5AUS3/NN7dN7p7j7v3dHV1tXpzAOpoNuxHzGyGJGXXQ+W1BKAVmg37Dkmrs9urJT1RTjsAWiV3nN3MHpO0TNI0Mzsk6aeS7pe01cxukvQXSd9tZZNnu23btiXrZpasf/DBB2W2E8ZHH31Ut3bRRRcl1x2Lbzlzw+7uK+qUvlVyLwBaiNNlgSAIOxAEYQeCIOxAEIQdCIKvuJbg2WefLbT+lCnpLw3ee++9hZ5/rCqy388///xkPe/f5GzEkR0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgmCcvQSDg4OF1l+yZEmyPmvWrELPP1bl7fe8aZlTnnvuuWR9+vS6v8QmSZo/f37T224VjuxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EATj7A365JNP6tYefvjhQs995513Flp/rErtc0navXt3sp76ie433ngjue6qVauS9YMHDybrnYgjOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EwTh7g/bs2VO39vzzzyfXvfzyy5P1pUuXNtVTGYaGhpL1J598MlnPm076xRdfrFt79913k+vmjbMfOHAgWU/Jmyb7sssuS9YnTJjQ9LarkntkN7PNZjZkZvtHLOszs7+Z2d7scm1r2wRQVCMv438l6epRlq9394XZ5ely2wJQttywu/tuSe+3oRcALVTkA7o1ZvZa9jK/7sRYZtZrZv1m1l+r1QpsDkARzYb9F5K+LmmhpEFJ6+o90N03unuPu/d0dXU1uTkARTUVdnc/4u6fuvtnkn4pKf3zqAAq11TYzWzGiLvfkbS/3mMBdIbccXYze0zSMknTzOyQpJ9KWmZmCyW5pAFJP2hhjx1h27ZtTa+7fPnyZP348ePJ+lNPPdX0th999NFkPe874SdPnmx621L6t9vzxrqLmjlzZt1a3n5ZtmxZyd1ULzfs7r5ilMWbWtALgBbidFkgCMIOBEHYgSAIOxAEYQeC4CuumQcffDBZ37BhQ91aaohHknbu3Jmsr127NllvpYsvvjhZ7+7uTtZnz56drF9xxRV1a5deemly3alTpybrefv9kksuqVsbi0NreTiyA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQjLNn8r7Cmvo6Zt7PKe/atStZnzhxYrI+d+7cZD011p03HfT8+fOT9UmTJiXrrfTmm28m63lfkW31V2jPNhzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiCIMOPsDz30ULL+wgsvNP3cH3/8cbJ+++23J+s33nhjsr5gwYIz7mksKPIT2vgyjuxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EESYcfY9e/Yk66dOnUrWzz333Lq1vHH0vr6+ZB2jmzNnTtUtjCm5R3Yzm2VmfzCzA2b2upn9MFs+1cx2mtlb2fWU1rcLoFmNvIw/JenH7v4Pkr4h6VYzmydpraRd7j5X0q7sPoAOlRt2dx9091ez28clHZB0oaTlkrZkD9si6fpWNQmguDP6gM7MuiUtkvQnSRe4+6A0/B+CpOl11uk1s34z66/VasW6BdC0hsNuZpMkbZf0I3c/1uh67r7R3Xvcvaerq6uZHgGUoKGwm9l4DQf9N+7+u2zxETObkdVnSBpqTYsAypA79GbDv8e7SdIBd//ZiNIOSasl3Z9dP9GSDkuyZcuWZH3lypXJ+lVXXVVmO2hAarpnSXL3QvVoGhlnv1LSSkn7zGxvtuwuDYd8q5ndJOkvkr7bmhYBlCE37O7+R0n1fm3/W+W2A6BVOF0WCIKwA0EQdiAIwg4EQdiBIMJ8xTUP4+idJ++My2XLliXrTNn8RRzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIxtnRscaNG5esT548OVnPm0o7Go7sQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAE4+w4a+VNlX3PPfe0qZOzA0d2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiikfnZZ0n6taS/l/SZpI3u/nMz65N0s6Ra9tC73P3pVjUKnG7p0qXJ+jPPPNOmTs4OjZxUc0rSj939VTP7qqRXzGxnVlvv7v/euvYAlKWR+dkHJQ1mt4+b2QFJF7a6MQDlOqP37GbWLWmRpD9li9aY2WtmttnMptRZp9fM+s2sv1arjfYQAG3QcNjNbJKk7ZJ+5O7HJP1C0tclLdTwkX/daOu5+0Z373H3nry5uwC0TkNhN7PxGg76b9z9d5Lk7kfc/VN3/0zSLyUtaV2bAIrKDbsNT4W5SdIBd//ZiOUzRjzsO5L2l98egLI08mn8lZJWStpnZnuzZXdJWmFmCyW5pAFJP2hJhwBK0cin8X+UNNpE14ypA2cRzqADgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EYe7evo2Z1ST9ecSiaZKOtq2BM9OpvXVqXxK9NavM3i5y91F//62tYf/Sxs363b2nsgYSOrW3Tu1Lordmtas3XsYDQRB2IIiqw76x4u2ndGpvndqXRG/Naktvlb5nB9A+VR/ZAbQJYQeCqCTsZna1mf2Pmb1tZmur6KEeMxsws31mttfM+ivuZbOZDZnZ/hHLpprZTjN7K7sedY69inrrM7O/Zftur5ldW1Fvs8zsD2Z2wMxeN7MfZssr3XeJvtqy39r+nt3Mxkn6X0n/LOmQpJclrXD3N9raSB1mNiCpx90rPwHDzL4p6YSkX7v7/GzZv0l6393vz/6jnOLuP+mQ3voknah6Gu9stqIZI6cZl3S9pH9Rhfsu0df31Ib9VsWRfYmkt939oLuflPRbScsr6KPjuftuSe+ftni5pC3Z7S0a/mNpuzq9dQR3H3T3V7PbxyV9Ps14pfsu0VdbVBH2CyX9dcT9Q+qs+d5d0u/N7BUz6626mVFc4O6D0vAfj6TpFfdzutxpvNvptGnGO2bfNTP9eVFVhH20qaQ6afzvSndfLOkaSbdmL1fRmIam8W6XUaYZ7wjNTn9eVBVhPyRp1oj7X5N0uII+RuXuh7PrIUmPq/Omoj7y+Qy62fVQxf38v06axnu0acbVAfuuyunPqwj7y5LmmtlsM5sg6fuSdlTQx5eY2XnZBycys/MkfVudNxX1Dkmrs9urJT1RYS9f0CnTeNebZlwV77vKpz9397ZfJF2r4U/k35H0r1X0UKevOZL+O7u8XnVvkh7T8Mu6TzT8iugmSX8naZekt7LrqR3U2yOS9kl6TcPBmlFRb0s1/NbwNUl7s8u1Ve+7RF9t2W+cLgsEwRl0QBCEHQiCsANBEHYgCMIOBEHYgSAIOxDE/wHK/S5/L/GtKQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the model.\n",
    "index = 12005\n",
    "x_in = x_test[index]\n",
    "plt.imshow(x_in, cmap='gray')\n",
    "preds = model.predict(np.array([x_in]))[0]\n",
    "pred_val = np.argmax(preds)\n",
    "print(f\"\"\"\n",
    "    Expected: {y_test[index]}\n",
    "    Predicted: {pred_val}\n",
    "    Probability: {preds[pred_val]:.2f}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This is a very simple model to illustrate Keras and Deep Learning. It will not work as well in the real world for couple of reasons:\n",
    "* Numbers may not be centered in the image or could be smaller relative to the image. \n",
    "* Background noise can be varied - lines, spots.\n",
    "\n",
    "For the simple purpose of illustrating building a deep learning model in Keras, I think it serves well.\n",
    "\n",
    "Next lets deploy this model and see how it does with recognizing real handwritten numbers!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
